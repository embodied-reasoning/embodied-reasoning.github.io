<!DOCTYPE html>
<html lang="en">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Embodied Reasoning in Action (ERA): Workshop and Challenge on Embodied Reasoning for Robotic Manipulation</title>
  <link rel="stylesheet" href="style.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L553BZYNH0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L553BZYNH0');
</script>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="./static/images/cvpr2026.png" width="150px" style="margin-right: 30px;">
      <a href="index.html#">Home</a>
      <a href="index.html#intro">Introduction</a>
      <a href="index.html#call">Challenges</a>
      <!-- <a href="index.html#challenge">Challenge</a> -->
      <a href="index.html#schedule">Schedule</a>
      <a href="index.html#speakers">Speakers</a>
      <a href="index.html#organizers">Organizers</a>
    </div>
  </div>

  <div class="title-container">
    <div style="text-align: center;" class="title-text">
      <div style="width: 90%; margin: auto;z-index: 2; position: relative;">
      <div class="subtitle">The first CVPR workshop on</div>
      <h2><b>Embodied Reasoning in Action (ERA)</b></h2>
      <div class="subtitle" style="margin: 20px;">
        Workshop and Challenge on Embodied Reasoning for Robotic Manipulation<br>
        June 3-4, 2026, Denver, CO.
      </div>
      </div>
    </div>
</div>

  <div class="container">
    <div class="section" id="intro">
      <h2>Introduction</h2>

      <p>
        Embodied reasoning—the ability to ground perception, language, and action in the physical world—is becoming
        central to robotic manipulation. Foundation models (LLMs, VLMs, and 3D vision models) bring broad world knowledge
        and compositional generalization, yet turning these capabilities into reliable, closed‑loop manipulation remains
        an open challenge: robots must reason about spatial relations, affordances, dynamics, and causality while adapting
        online to novel scenes, objects, and tasks. This workshop convenes researchers across robotics, embodied AI,
        perception, planning, control, simulation, and sim‑to‑real to chart the next steps in reasoning‑centric manipulation.<br><br>
        
        We introduce two community challenges to benchmark progress: <b>RoboSpatial</b>, evaluates a model's ability to understand
        spatial relationships—where to point, fit, and place—across different camera views and scenes, and <b>PointArena (Point‑Bench)</b>,
        Measures precise, language‑guided pointing by asking models to select the correct pixel(s) in an image with automated scoring.
        Together, these tracks measure generalization across viewpoints, sensors, and object sets; robustness to environmental
        perturbations; and the fidelity of language‑to‑action grounding.<br><br>

        By bringing together diverse perspectives—from algorithmic foundations and datasets to hardware‑aware systems and
        deployments—we aim to identify what forms of representation, learning, and planning truly scale embodied reasoning
        for real‑world manipulation, and to catalyze collaboration between academia and industry.
      </p>


    </div>

    <div class="section" id="call">
      <h2>Challenges</h2>
 <p>
    We are excited to announce the challenges for the ERA workshop.
  </p>
    
  <h3>RoboSpatial Challenge</h3>
  <p><a href="https://arxiv.org/abs/2411.16537">RoboSpatial</a> evaluates a model’s embodied spatial reasoning 
    by testing its ability to infer where to point, fit, and place from real-world RGB-D observations. It contains
    350 spatial question–answer pairs collected across five real apartments, with both RGB and depth data captured 
    using an iPhone 13 Pro Max. Each scene is annotated under three reasoning types: configuration (object relations), 
    context (free-space reasoning), and compatibility (affordance and fit). By assessing how models interpret 3D 
    spatial structure and generalize across diverse indoor scenes and viewpoints, RoboSpatial provides a diagnostic 
    benchmark for grounding perception and language in actionable spatial understanding, and is increasingly adopted 
    by frontier models such as Qwen3-VL and Gemini Robotics for evaluating embodied spatial reasoning.</p>
  <img src="static/images/robospatial.png" alt="RoboSpatial" style="max-width: 100%; height: auto;"> <br><br>

  <h3>PointArena Challenge</h3>
  <p><a href="https://arxiv.org/abs/2505.09990">Colosseum</a> aims to evaluate models' generalization across various scene perturbations. It encompasses 14 perturbation 
    factors within 20 distinct RLBench tasks, categorized into three tiers (simple, intermediate, and complex) according to 
    the number of way-points involved (task horizon). Collectively, Colosseum presents 20,371 unique task perturbations instances.</p>
  <img src="static/images/pointarena.png" alt="PointArena" style="max-width: 100%; height: auto;">

  <h2>Important Dates</h2>
  <table>
    <thead>
      <tr>
        <th>Event</th>
        <th>Date</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Challenges</strong></td>
        <td>TBD</td>
      </tr>
      <tr>
        <td><strong>Submission Deadline</strong></td>
        <td>TBD</td>
      </tr>
      <tr>
        <td><strong>Notification</strong></td>
        <td>TBD</td>
      </tr>
      <tr>
        <td><strong>Camera-Ready</strong></td>
        <td>TBD</td>
      </tr>
    </tbody>
  </table>

  <h2>Submission Guidelines</h2>
  <ul>
    <li><strong>Page Limit:</strong> Submissions can be up to 4 pages for the main content. There is no limit on the number of pages for references or appendices.</li>
    <li><strong>Formatting:</strong> Submissions are encouraged to use the CVPR template.</li>
    <li><strong>Anonymity:</strong> All submissions must be anonymized. Please remove any author names, affiliations, or identifying information.</li>
    <li><strong>Relevant Work:</strong> We welcome references to recently published, relevant work (e.g., RSS, CoRL, ICRA, and ICML).</li>
    <li><strong>Archival Status:</strong> All accepted papers are non-archival.</li>
<li><strong>Link:</strong> <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/Robo-3Dvlm&referrer=%5BHomepage%5D(%2F)#tab-your-consoles">openreview submission</a></li>
        <p>
    Accepted papers will be presented in the form of posters at the workshop. In addition, selected papers may be invited to deliver spotlight talks.
  </p>
  </ul>
      </p>
      <h3>Paper topics</h3>
      <p>
      A non-exhaustive list of relevant topics:
      <ul>
        <li>3D Vision-Language Policy Learning</li>
        <li>Pretraining for 3D Vision-Language Models</li>
        <li>3D Representations for Policy Learning (i.e. NeRF, Gaussian Splatting, SDF)</li>
        <li>3D Benchmarks and Simulatotion frameworks</li>
        <li>3D Vision-Language Action Models</li>
        <li>3D Vision-Language or Large-Language Models for Robotics</li>
        <li>3D Instruction-tuning datasets for Robotics</li>
        <li>3D pretraining datasets for Robotics</li>
        <li>Other topics about 3D Vision-Language Models for Robotic Manipulations</li>
      </ul>
      </p>


    <div class="section" id="schedule">
      <h2>Workshop Schedule (Tentative)</h2>
      <style type="text/css">
        .tg .tg-u4qn {
          background-color: #D9D9D9;
          text-align: left;
          vertical-align: bottom
        }

        .tg-za14 {
          width: 600px;
        }

        /* .tg .tg-7zrl{text-align:left;vertical-align:bottom} */
      </style>
      <table class="tg">
        <thead>
          <tr>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Start Time (CDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">End Time (CDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Event</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-jkyp">9:00 AM</td>
            <td class="tg-jkyp">9:10 AM</td>
            <td class="tg-za14">Opening remarks</td>
          </tr>

          <tr>
            <td class="tg-jkyp">9:10 AM</td>
            <td class="tg-jkyp">9:45 AM</td>
            <td class="tg-za14"><b>Hao Su </b> <br/> Exploring World Model for Robotic Manipulation</td>
          </tr>

          <tr>
            <td class="tg-jkyp">9:45 AM</td>
            <td class="tg-jkyp">10:20 AM</td>
            <td class="tg-za14"><b>Chelsea Finn</b> <br/> Pretraining and Posttraining Robotic Foundation Models</td>
          </tr>

          <tr>
            <td class="tg-jkyp">10:20 AM</td>
            <td class="tg-jkyp">10:55 AM</td>
            <td class="tg-za14"><b>Ranjay Krishna</b> <br/> Preparing perception for robotics</td>
          </tr>

          <tr>
            <td class="tg-jkyp">10:55 AM</td>
            <td class="tg-jkyp">11:10 AM</td>
            <td class="tg-za14">Coffee Break</td>
          </tr>

          <tr>
            <td class="tg-jkyp">11:10 AM</td>
            <td class="tg-jkyp">11:45 AM</td>
            <td class="tg-za14"><b>Yunzhu Li</b> <br> Foundation Models for Structured Scene Modeling in Robotic Manipulation</td>
          </tr>

          <tr>
            <td class="tg-jkyp">11:45 AM</td>
            <td class="tg-jkyp">12:20 PM</td>
            <td class="tg-za14"><b>Katerina Fragkiada</b> <br/> 3D Generative Manipulation Policies: Bridging 2D Pre-training with 3D Scene Reasoning</td>
          </tr>

          <tr>
            <td class="tg-jkyp">12:20 PM</td>
            <td class="tg-jkyp">1:30 PM</td>
            <td class="tg-za14"><b>Lunch</td>
          </tr>

          <tr>
            <td class="tg-jkyp">1:30 PM</td>
            <td class="tg-jkyp">2:00 PM</td>
            <td class="tg-za14">Poster Session (ExHall D, #357-#371</td>
          </tr>


          <tr>
            <td class="tg-jkyp">2:00 PM</td>
            <td class="tg-jkyp">2:35 PM</td>
            <td class="tg-za14"><b>Angel Chang</b> <br/> Building vision-language maps for embodied AI</td>
          </tr>

          <tr>
            <td class="tg-jkyp">2:35 PM</td>
            <td class="tg-jkyp">3:10 PM</td>
            <td class="tg-za14"><b>Dieter Fox</b> <br/> Hierarchical Action Models for Open-World 3D Policies </td>
          </tr>

          <tr>
            <td class="tg-jkyp">3:10 PM</td>
            <td class="tg-jkyp">3:25 PM</td>
            <td class="tg-za14"><b>Coffee Break</td>
          </tr>

          <tr>
            <td class="tg-jkyp">3:25 PM</td>
            <td class="tg-jkyp">4:00 PM</td>
            <td class="tg-za14"><b>Chuang Gan</b> <br/> Genesis: An Unified and Generative Physics Simulation for Robotics</td>
          </tr>

            <tr>
              <td class="tg-jkyp">4:00 PM</td>
              <td class="tg-jkyp">4:45 PM</td>
              <td class="tg-za14">
                <b>Spotlight Paper Talks&nbsp;(5 min talk&nbsp;/ 2 min Q&A)</b><br>
                &bull; The One RING: A Robotic Indoor Navigation Generalist<br>
                &bull; Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models<br>
                &bull; Agentic Language-Grounded Adaptive Robotic Assembly<br>
                &bull; ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos
              </td>
            </tr>


          <tr>
            <td class="tg-jkyp">4:45 PM</td>
            <td class="tg-jkyp">5:00 PM</td>
            <td class="tg-za14"><b>Ending Remarks and Paper Awards</td>
          </tr>

        </tbody>
      </table>

    </div>

    <div class="section" id="speakers">
      <h2>Invited Speakers</h2>
      <p> listed alphabetically </p>
      <div class="people">



        <a href="https://angelxuanchang.github.io/">
          <img src="./static/images/angel.jpg">
          <div>Angel Chang</div>
          <div class="aff">Simon Fraser University</div>
        </a>


       <a href="https://ai.stanford.edu/~cbfinn/">
          <img src="./static/images/chelsea.jpg">
          <div>Chelsea Finn</div>
          <div class="aff">Stanford University</div>
        </a>

        <a href="https://people.csail.mit.edu/ganchuang/">
          <img src="./static/images/chuang.jpg">
          <div>	Chuang Gan</div>
          <div class="aff">UMass Amherst</div>
        </a>

        <a href="https://cseweb.ucsd.edu//~haosu/">
          <img src="./static/images/haosu.jpeg">
          <div>Hao Su</div>
          <div class="aff">UC San Diego</div>
        </a>

        <a href="https://www.cs.cmu.edu/~katef/">
          <img src="./static/images/katerina.png">
          <div>Katerina Fragkiadaki</div>
          <div class="aff">Carnegie Mellon University</div>
        </a>

        <a href="https://yunzhuli.github.io/">
          <img src="./static/images/yunzhu.jpg">
          <div>Yunzhu Li</div>
          <div class="aff">Columbia University</div>
        </a>


        <a href="http://ranjaykrishna.com/index.html">
          <img src="./static/images/ranjay.jpeg">
          <div>Ranjay Krishna</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://homes.cs.washington.edu/~fox/">
          <img src="./static/images/fox-2017.jpg">
          <div>Dieter Fox</div>
          <div class="aff">University of Washington</div>
        </a>
   

      </div>
    </div>


    <div class="section" id="organizers">
      <h2>Organizers</h2>
      <!-- <p> listed alphabetically </p> -->
      <div class="people">
        <a href="https://duanjiafei.com/">
          <img src="./static/images/jiafei.png">
          <div>Jiafei Duan</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://zubairirshad.com">
          <img src="./static/images/zubair.JPG">
          <div>Zubair Irshad</div>
          <div class="aff">Toyota Research Institute</div>
        </a>

        <a href="https://ishikasingh.github.io/">
          <img src="./static/images/ishika.jpg">
          <div>Ishika Singh</div>
          <div class="aff">USC</div>
        </a>

        <a href="https://www.linkedin.com/in/vitorguizilini/">
          <img src="./static/images/vitor.jpg">
          <div>Vitor Guizilini</div>
          <div class="aff">Toyota Research Institute</div>
        </a>

        <a href="https://www.tri.global/about-us/dr-rares-ambrus">
          <img src="./static/images/rares.jpg">
          <div>Rares Ambrus</div>
          <div class="aff">Toyota Research Institute</div>
        </a>

        <a href="https://faculty.cc.gatech.edu/~zk15/">
          <img src="./static/images/zsolt.jpg">
          <div>Zsolt Kira</div>
          <div class="aff">Georgia Institute of Technology</div>
        </a>

        <a href="https://hq-fang.github.io/">
          <img src="./static/images/haoquan.jpg">
          <div>Haoquan Fang</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://jasonlee328.github.io/">
          <img src="./static/images/jason.jpg">
          <div>Jason Lee</div>
          <div class="aff">University of Washington</div>
        </a>
      </div>
    </div>


<div class="foot">
    The website template is borrowed from <a href="https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/">here</a>. 
    <br>
    For inquiries, contact us at: <a href="mailto:robo-3dvlm@googlegroups.com">robo-3dvlm@googlegroups.com</a>
</div>

</body>

</html>
