<!DOCTYPE html>
<html lang="en">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Embodied Reasoning in Action (ERA): Workshop and Challenge on Embodied Reasoning for Robotic Manipulation</title>
  <link rel="stylesheet" href="style.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L553BZYNH0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L553BZYNH0');
</script>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="./static/images/cvpr2026.png" width="150px" style="margin-right: 30px;">
      <a href="index.html#">Home</a>
      <a href="index.html#intro">Introduction</a>
      <a href="index.html#call">Challenges</a>
      <!-- <a href="index.html#challenge">Challenge</a> -->
      <a href="index.html#schedule">Schedule</a>
      <a href="index.html#speakers">Speakers</a>
      <a href="index.html#organizers">Organizers</a>
    </div>
  </div>

  <div class="title-container">
    <div style="text-align: center;" class="title-text">
      <div style="width: 90%; margin: auto;z-index: 2; position: relative;">
      <div class="subtitle">The first CVPR workshop on</div>
      <h2><b>Embodied Reasoning in Action (ERA)</b></h2>
      <div class="subtitle" style="margin: 20px;">
        Workshop and Challenge on Embodied Reasoning for Robotic Manipulation<br>
        June 3-4, 2026, Denver, CO.
      </div>
      </div>
    </div>
</div>

  <div class="container">
    <div class="section" id="intro">
      <h2>Introduction</h2>

      <p>
        Embodied reasoning—the ability to ground perception, language, and action in the physical world—is becoming
        central to robotic manipulation. Foundation models (LLMs, VLMs, and 3D vision models) bring broad world knowledge
        and compositional generalization, yet turning these capabilities into reliable, closed‑loop manipulation remains
        an open challenge: robots must reason about spatial relations, affordances, dynamics, and causality while adapting
        online to novel scenes, objects, and tasks. This workshop convenes researchers across robotics, embodied AI,
        perception, planning, control, simulation, and sim‑to‑real to chart the next steps in reasoning‑centric manipulation.<br><br>
        
        We introduce two community challenges to benchmark progress: <b>RoboSpatial</b>, evaluates a model's ability to understand
        spatial relationships—where to point, fit, and place—across different camera views and scenes, and <b>PointArena (Point‑Bench)</b>,
        Measures precise, language‑guided pointing by asking models to select the correct pixel(s) in an image with automated scoring.
        Together, these tracks measure generalization across viewpoints, sensors, and object sets; robustness to environmental
        perturbations; and the fidelity of language‑to‑action grounding.<br><br>

        By bringing together diverse perspectives—from algorithmic foundations and datasets to hardware‑aware systems and
        deployments—we aim to identify what forms of representation, learning, and planning truly scale embodied reasoning
        for real‑world manipulation, and to catalyze collaboration between academia and industry.
      </p>


    </div>

    <div class="section" id="call">
      <h2>Challenges</h2>
 <p>
    We are excited to announce the challenges for the ERA workshop.
  </p>
    
  <h3>RoboSpatial Challenge</h3>
  <p><a href="https://arxiv.org/abs/2411.16537">RoboSpatial</a> evaluates a model’s embodied spatial reasoning 
    by testing its ability to infer where to point, fit, and place from real-world RGB-D observations. It contains
    350 spatial question–answer pairs collected across five real apartments, with both RGB and depth data captured 
    using an iPhone 13 Pro Max. Each scene is annotated under three reasoning types: configuration (object relations), 
    context (free-space reasoning), and compatibility (affordance and fit). By assessing how models interpret 3D 
    spatial structure and generalize across diverse indoor scenes and viewpoints, RoboSpatial provides a diagnostic 
    benchmark for grounding perception and language in actionable spatial understanding, and is increasingly adopted 
    by frontier models such as Qwen3-VL and Gemini Robotics for evaluating embodied spatial reasoning.</p>
  <img src="static/images/robospatial.png" alt="RoboSpatial" style="max-width: 100%; height: auto;"> <br><br>

  <h3>PointArena Challenge</h3>
  <p><a href="https://arxiv.org/abs/2505.09990">PointArena</a> is the largest benchmark for evaluating language-guided 
    pointing, comprising 982 text-image pairs with pixel-level target masks collected from public sources after April 20, 2025. 
    The dataset is evenly divided into five task-driven categories—Spatial, Affordance, Counting, Steerable, and Reasoning—derived 
    from a survey of question types frequently tackled by open-source MLLMs. Each category targets a distinct capability: 1) Spatial 
    focuses on positional queries within scenes rich in spatial relationships or repeated objects (e.g., “Point to the leftmost
     tree in the image”); 2) Affordance emphasizes functional parts of objects, typically in tabletop scenes, prompting queries 
     like “Point to the handle used for pouring”; 3) Counting features multiple similar items and supports queries about subsets 
     based on number or attributes, such as “Point to all the blue cars in the image”; 4) Steerable leverages images from the PixMo 
     dataset that include a reference point, guiding annotators to ask relative-position questions like “Point to the item closest to 
     the marked point”; and 5) Reasoning presents event-rich or abstract scenes, inviting open-ended queries that require inference, 
     such as “Point to the tallest man-made object in the image.” </p>
  <img src="static/images/pointarena.png" alt="PointArena" style="max-width: 100%; height: auto;">

  <h2>Important Dates</h2>
  <table>
    <thead>
      <tr>
        <th>Event</th>
        <th>Date</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Challenge Launch</strong></td>
        <td>January 2026 (following workshop acceptance)</td>
      </tr>
      <tr>
        <td><strong>Simulation Phase Deadline</strong></td>
        <td>April 30, 2026</td>
      </tr>
      <tr>
        <td><strong>Verification Period</strong></td>
        <td>May 2026</td>
      </tr>
      <tr>
        <td><strong>Winner Notifications</strong></td>
        <td>End of May 2026</td>
      </tr>
    </tbody>
  </table> <br>

  <h2>Evaluation Guidelines</h2>
    <p><b>Evaluation.</b> The two challenges will be evaluated independently. The primary metric is the average success rate across 
      all tasks. Each challenge includes a public test split, which has already been released with the benchmark. Because participants 
      can self-evaluate and post results to the leaderboards for both benchmarks, we will require submission of model checkpoints for 
      verification. To minimize leakage of test information, each team will be limited to a small number of submissions.</p>
    <p><b>Prizes.</b> Final rankings will be determined by benchmark performance results. Prizes will be awarded to the winners, 
      contingent on sponsor availability.</p> 
    <p>We will consider having posters from the challenge participants and invited posters from the main conference papers that are relevant, 
      but will not hold a paper submission.</p>
    <br>




    <div class="section" id="schedule">
      <h2>Workshop Schedule (Tentative)</h2>
      <style type="text/css">
        .tg .tg-u4qn {
          background-color: #D9D9D9;
          text-align: left;
          vertical-align: bottom
        }

        .tg-za14 {
          width: 600px;
        }

        /* .tg .tg-7zrl{text-align:left;vertical-align:bottom} */
      </style>
      <table class="tg">
        <thead>
          <tr>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Start Time (CDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">End Time (CDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Event</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-jkyp">9:00 AM</td>
            <td class="tg-jkyp">9:10 AM</td>
            <td class="tg-za14">Introduction and welcome</td>
          </tr>

          <tr>
            <td class="tg-jkyp">9:10 AM</td>
            <td class="tg-jkyp">9:45 AM</td>
            <td class="tg-za14"><b>Invited speaker: Furong Huang</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">9:45 AM</td>
            <td class="tg-jkyp">10:20 AM</td>
            <td class="tg-za14"><b>Invited speaker: Tianmin Shu</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">10:20 AM</td>
            <td class="tg-jkyp">10:55 AM</td>
            <td class="tg-za14"><b>Invited speaker: Ranjay Krishna</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">10:55 AM</td>
            <td class="tg-jkyp">11:10 AM</td>
            <td class="tg-za14">Coffee break (Tentative Poster Session)</td>
          </tr>

          <tr>
            <td class="tg-jkyp">11:10 AM</td>
            <td class="tg-jkyp">11:45 AM</td>
            <td class="tg-za14"><b>Invited speaker: Cheng Chi</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">11:45 AM</td>
            <td class="tg-jkyp">12:20 PM</td>
            <td class="tg-za14">Challenge overview and results summary<br></td>
          </tr>

          <tr>
            <td class="tg-jkyp">12:20 PM</td>
            <td class="tg-jkyp">12:30 PM</td>
            <td class="tg-za14">Winner and runner-up presentations for RoboSpatial and Point-Bench challenges</td>
          </tr>

          <tr>
            <td class="tg-jkyp">12:30 PM</td>
            <td class="tg-jkyp">2:00 PM</td>
            <td class="tg-za14">Lunch</td>
          </tr>

          <tr>
            <td class="tg-jkyp">2:00 PM</td>
            <td class="tg-jkyp">2:35 PM</td>
            <td class="tg-za14">Spotlight Presentation (15mins each)</td>
          </tr>

          <tr>
            <td class="tg-jkyp">2:35 PM</td>
            <td class="tg-jkyp">3:10 PM</td>
            <td class="tg-za14"><b>Invited speaker: Jiajun Wu</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">3:10 PM</td>
            <td class="tg-jkyp">3:25 PM</td>
            <td class="tg-za14"><b>Invited speaker: Karl Pertsch</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">3:25 PM</td>
            <td class="tg-jkyp">4:00 PM</td>
            <td class="tg-za14"><b>Invited speaker: Yilun Du</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">4:00 PM</td>
            <td class="tg-jkyp">4:45 PM</td>
            <td class="tg-za14">Panel Discussion</td>
          </tr>

          <tr>
            <td class="tg-jkyp">4:45 PM</td>
            <td class="tg-jkyp">5:00 PM</td>
            <td class="tg-za14">Award ceremony and closing remark</td>
          </tr>

        </tbody>
      </table>

    </div>

    <div class="section" id="speakers">
      <h2>Invited Speakers</h2>
      <p> listed alphabetically </p>
      <div class="people">

        <a href="https://chicheng123.github.io/">
          <img src="./static/images/chengchi.jpg">
          <div>Cheng Chi</div>
          <div class="aff">Beijing Academy of Artificial Intelligence</div>
        </a>

        <a href="https://yilundu.github.io/">
          <img src="./static/images/yilundu.png">
          <div>Yilun Du</div>
          <div class="aff">Harvard University</div>
        </a>

        <a href="https://furong-huang.com/">
          <img src="./static/images/furonghuang.jpeg">
          <div>Furong Huang</div>
          <div class="aff">University of Maryland</div>
        </a>

        <a href="http://ranjaykrishna.com/index.html">
          <img src="./static/images/ranjaykrishna.jpeg">
          <div>Ranjay Krishna</div>
          <div class="aff">University of Washington / Ai2</div>
        </a>

        <a href="https://kpertsch.github.io/">
          <img src="./static/images/karlpertsch.jpg">
          <div>Karl Pertsch</div>
          <div class="aff">Physical Intelligence</div>
        </a>

        <a href="https://www.tshu.io/">
          <img src="./static/images/tianminshu.jpg">
          <div>Tianmin Shu</div>
          <div class="aff">Johns Hopkins University</div>
        </a>

        <a href="https://jiajunwu.com/">
          <img src="./static/images/jiajunwu.jpg">
          <div>Jiajun Wu</div>
          <div class="aff">Stanford University</div>
        </a>

      </div>
    </div>


    <div class="section" id="organizers">
      <h2>Organizers</h2>
      <!-- <p> listed alphabetically </p> -->
      <div class="people">
        <a href="https://duanjiafei.com/">
          <img src="./static/images/jiafei.png">
          <div>Jiafei Duan</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://hq-fang.github.io/">
          <img src="./static/images/haoquan.jpg">
          <div>Haoquan Fang</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://homes.cs.washington.edu/~fox/">
          <img src="./static/images/fox-2017.jpg">
          <div>Dieter Fox</div>
          <div class="aff">University of Washington / Ai2</div>
        </a>

        <a href="https://mayasguru.github.io/">
          <img src="./static/images/mayaguru.jpg">
          <div>Maya Guru</div>
          <div class="aff">Ai2</div>
        </a>

        <a href="#">
          <img src="./static/images/yejinkim.jpg">
          <div>Yejin Kim</div>
          <div class="aff">Ai2</div>
        </a>

        <a href="https://mars-tin.github.io/">
          <img src="./static/images/martin.jpeg">
          <div>Martin Ziqiao Ma</div>
          <div class="aff">University of Michigan</div>
        </a>

        <a href="https://jason718.github.io/">
          <img src="./static/images/jasonren.jpeg">
          <div>Jason Ren</div>
          <div class="aff">University of North Carolina at Chapel Hill / Ai2</div>
        </a>

        <a href="https://chanh.ee/">
          <img src="./static/images/lukesong.jpg">
          <div>Chan Hee (Luke) Song</div>
          <div class="aff">The Ohio State University</div>
        </a>

        <a href="https://helen9975.github.io/">
          <img src="./static/images/helenwang.jpg">
          <div>Helen Wang</div>
          <div class="aff">University of Washington</div>
        </a>

        <a href="https://wentaoyuan.github.io/">
          <img src="./static/images/wentaoyuan.jpg">
          <div>Wentao Yuan</div>
          <div class="aff">Google DeepMind Robotics</div>
        </a>
      </div>
    </div>


<div class="foot">
    The website template is borrowed from <a href="https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/">here</a>. 
    <br>
    For inquiries, contact us at: <a href="mailto:robo-3dvlm@googlegroups.com">robo-3dvlm@googlegroups.com</a>
</div>

</body>

</html>
